{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c557851",
   "metadata": {},
   "source": [
    "# 모델 선정\n",
    "1. 영화 리뷰 데이터셋의 특성을 고려하여 KoBERT, KoELECTA 중 구어체, 메신저, 웹 데이터가 포함된 데이터셋을 훈련시킨 KoELECTRA 모델을 선정\n",
    "2. KoBERT, KoELECTRA 모델을 비교 실험한 결과 KoELECTRA 모델의 성능이 우수하며 훈련 시간 또한 단축됨\n",
    "\n",
    "# 훈련 시킬 데이터셋 구축\n",
    "1. EDA에서 발견한 데이터 불균형을 고려하여, 각 클래스마다 동일한 개수의 데이터를 사용하여 데이터셋 구축\n",
    "2. 각 클래스 별 사용 데이터의 개수를 늘리면서 실험 진행\n",
    "3. 중복 데이터는 제외한 후 훈련\n",
    "4. EDA에서 정수 인코딩 된 문장의 길이가 32 이하인 문장이 전체의 약 86% 이며, 훈련 시간을 고려하여 sequence 길이는 32로 패딩\n",
    "\n",
    "# 실험 및 결과\n",
    "1. 각각의 클래스마다 동일한 개수의 데이터를 사용하여 submission 제출 결과 0.37의 점수 도출\n",
    "2. 클래스 별 사용 데이터의 개수를 최대 300만개 까지 늘린 후 submission 제출 결과 2 epoch에서 0.604의 점수 도출 -> TestSet 또한 데이터 불균형이 있을 가능성을 확인\n",
    "3. 데이터 중복 처리를 한 결과가 그렇지 않은 결과보다 성능이 향상됨\n",
    "4. sequence 길이를 64로 패딩할 때와 32로 패딩할 때를 비교한 결과 성능은 비슷하지만, 32로 패딩할 때의 훈련 시간이 단축됨 -> 모델이 입력 문장 전체를 보고 판단할 필요는 없음을 확인\n",
    "5. scheduler의 적용 여부를 실험한 결과, 적용하지 않은 모델의 성능이 우수\n",
    "\n",
    "# 추후 개선 가능성\n",
    "1. 전체 데이터셋을 사용하여 훈련하지 않았으며, 데이터의 수가 늘어날수록 성능이 향상되는 양상을 보임\n",
    "2. 다양한 scheduler를 통해 실험한다면 더 나은 결과를 보일 가능성이 있음\n",
    "3. weight decay 적용을 통한 과적합 해결 기대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe044d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers.__version__ 4.6.1\n",
      "mxnet.__version__ 1.7.0\n",
      "len(set(train_y)) 10 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "len(set(valid_y)) 10 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "len(trainSet) 7579756\n",
      "len(validSet) 955010\n",
      "preprocess\n",
      "6809008\n",
      "955010\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n",
      "create dataset\n",
      "loop start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d8493fed1297>:218: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c808091a1b04bc39e7a6dd8deb70f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 2.34513783454895 train acc 0.1\n",
      "epoch 1 batch id 3001 loss 1.2640464305877686 train acc 0.5491945774172596\n",
      "epoch 1 batch id 6001 loss 1.1447519063949585 train acc 0.5577541954389714\n",
      "epoch 1 train acc 0.5611302977508213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-d8493fed1297>:240: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782a68fb57194bf2955ef185b3ade336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 valid loss 1.07378880286933 acc 0.6214429725720871\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c46d7120a247f9b16791b99aaf87b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 1.188226580619812 train acc 0.6048780487804878\n",
      "epoch 2 batch id 3001 loss 1.1998754739761353 train acc 0.5759425719881766\n",
      "epoch 2 batch id 6001 loss 1.163714051246643 train acc 0.576409622786448\n",
      "epoch 2 train acc 0.5764831755581094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48518ec7fc7b4b5781241ef93fd3d9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 valid loss 1.071791572683359 acc 0.6216310197922628\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4921c4c285bc47e5b302cb3e3d4187a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 1.146508812904358 train acc 0.5975609756097561\n",
      "epoch 3 batch id 3001 loss 1.17511785030365 train acc 0.582225030680806\n",
      "epoch 3 batch id 6001 loss 1.1705378293991089 train acc 0.5824147194979672\n",
      "epoch 3 train acc 0.582276657094512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32628191c7104508a64edd01d49e672a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 valid loss 1.0645165302211124 acc 0.6233195933708473\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1342771fbf664f56b3058101ef5ae724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 1.116074800491333 train acc 0.5817073170731707\n",
      "epoch 4 batch id 3001 loss 1.0890370607376099 train acc 0.5874846595849914\n",
      "epoch 4 batch id 6001 loss 1.1176083087921143 train acc 0.587340321328555\n",
      "epoch 4 train acc 0.5871290871230153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ddf8bf0b2847278a3d91adff298892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 valid loss 1.0689687622463242 acc 0.623153153509024\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c547d0fb04b045acb89944940ffda2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 1.109312891960144 train acc 0.6085365853658536\n",
      "epoch 5 batch id 3001 loss 1.0845779180526733 train acc 0.5929665721182106\n",
      "epoch 5 batch id 6001 loss 1.124261498451233 train acc 0.5921029421925553\n",
      "epoch 5 train acc 0.5916660556499522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23905ae207ce468c92ba7a169c197dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 valid loss 1.0787487955052453 acc 0.6219139300308717\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ad22931bbb4e98874191df54490ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 1.0802961587905884 train acc 0.5963414634146341\n",
      "epoch 6 batch id 3001 loss 1.0648103952407837 train acc 0.5974459732934285\n",
      "epoch 6 batch id 6001 loss 1.0998690128326416 train acc 0.5965786190106455\n",
      "epoch 6 train acc 0.596200813797095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fea890b0ba43d692297b724e9cdb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 valid loss 1.0888213877514197 acc 0.6196568696191476\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b701596698c8444595ea5a71f0ea1bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 1.0378354787826538 train acc 0.6060975609756097\n",
      "epoch 7 batch id 3001 loss 1.0458824634552002 train acc 0.6018347542688768\n",
      "epoch 7 batch id 6001 loss 1.0948565006256104 train acc 0.6010475896293637\n",
      "epoch 7 train acc 0.6005545780717311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ce5ae853184007b115ea12b71dc975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 valid loss 1.1035275179940744 acc 0.6171889103294659\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd0504abe6f417e95566b7185669150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 1.091403841972351 train acc 0.5926829268292683\n",
      "epoch 8 batch id 3001 loss 1.1037347316741943 train acc 0.6060874017603651\n",
      "epoch 8 batch id 6001 loss 1.0114260911941528 train acc 0.6055185924297081\n",
      "epoch 8 train acc 0.6051211423191284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f435898958f4d608a22afa700fc1176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 valid loss 1.1188026595524965 acc 0.6120692304501846\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cbe19f31884ab7868042aea1240b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 1.0141222476959229 train acc 0.6097560975609756\n",
      "epoch 9 batch id 3001 loss 1.0666812658309937 train acc 0.6125458180606278\n",
      "epoch 9 batch id 6001 loss 0.9962368607521057 train acc 0.6107435346141441\n",
      "epoch 9 train acc 0.6098035692723996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879ad6ff38854f8e9c069a936f28dcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 valid loss 1.1275187568603156 acc 0.612319887657542\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92f6c94b2fe4bfcaf14f8dd99c0db4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 1.0461211204528809 train acc 0.598780487804878\n",
      "epoch 10 batch id 3001 loss 1.0373928546905518 train acc 0.6176530587365047\n",
      "epoch 10 batch id 6001 loss 0.9912645220756531 train acc 0.6160324905198683\n",
      "epoch 10 train acc 0.6149914142502967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c8d0c2a9234a03939e01ae015893e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 valid loss 1.1417690326727512 acc 0.6118533938760765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mxnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from transformers import AdamW\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "print(\"transformers.__version__\", transformers.__version__)\n",
    "print(\"mxnet.__version__\", mxnet.__version__)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Seed 고정\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# ============================================================================\n",
    "# Load data\n",
    "train_x = []\n",
    "train_y = []\n",
    "valid_x = []\n",
    "valid_y = []\n",
    "with open(\"./inputs/train_data\", 'rt', encoding='UTF-8') as f:\n",
    "    for e, i in enumerate(f):        \n",
    "        if 8000000 < e < 9000000:\n",
    "            valid_x.append(i[:-1])\n",
    "        else:\n",
    "            train_x.append(i[:-1])\n",
    "with open(\"./inputs/train_label\", 'rt', encoding='UTF-8') as f:\n",
    "    for e, i in enumerate(f):        \n",
    "        if 8000000 < e < 9000000:\n",
    "            valid_y.append(int(i[:-1]) - 1)\n",
    "        else:\n",
    "            train_y.append(int(i[:-1]) - 1)\n",
    "\n",
    "print(\"len(set(train_y))\", len(set(train_y)), set(train_y))\n",
    "print(\"len(set(valid_y))\", len(set(valid_y)), set(valid_y))\n",
    "\n",
    "trainSet = []\n",
    "validSet = []\n",
    "\n",
    "for i, j in zip(train_x, train_y):\n",
    "    trainSet.append((i, j))\n",
    "for i, j in zip(valid_x, valid_y):\n",
    "    validSet.append((i, j))\n",
    "\n",
    "# ============================================================================\n",
    "# 데이터셋 중복 제거\n",
    "    \n",
    "trainSet = list(set(trainSet))  \n",
    "validSet = list(set(validSet))\n",
    "\n",
    "print(\"len(trainSet)\", len(trainSet))\n",
    "print(\"len(validSet)\", len(validSet))\n",
    "\n",
    "# ============================================================================\n",
    "# 클래스 별 사용할 데이터 개수 최대값 지정 후 추출 -> 데이터셋 생성\n",
    "print(\"preprocess\")\n",
    "\n",
    "dic = {}\n",
    "for i in range(10):\n",
    "    dic[i] = []\n",
    "for i in trainSet:\n",
    "    if len(dic[i[1]]) < 3000000:\n",
    "        dic[i[1]].append(i)\n",
    "\n",
    "trainSet = []\n",
    "for i in dic:\n",
    "    for j in dic[i]:\n",
    "        trainSet.append(j)\n",
    "        \n",
    "print(len(trainSet))\n",
    "print(len(validSet))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(device)\n",
    "\n",
    "# ============================================================================\n",
    "# Load model, tokenizer\n",
    "electra_model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "print(\"load model\")\n",
    "\n",
    "# ============================================================================\n",
    "# Setting parameters\n",
    "max_len = 32\n",
    "batch_size = 820\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 3000\n",
    "learning_rate = 5e-5\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 모델 입력을 위한 패딩 함수 \n",
    "def padding(inputs, pad_token, pad_length=0, pad=True):\n",
    "    pad_token = pad_token[0]\n",
    "\n",
    "    if pad_length > len(inputs):\n",
    "        if pad is True:\n",
    "            length = (pad_length - len(inputs))\n",
    "            pad_seq = [pad_token] * length\n",
    "            inputs.extend(pad_seq)\n",
    "\n",
    "    else:\n",
    "        if pad is True:\n",
    "            inputs = inputs[:pad_length]\n",
    "    return np.array(inputs)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Torch Dataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, electra_tokenizer, max_len, pad):\n",
    "        self.sentences = [electra_tokenizer.convert_tokens_to_ids(electra_tokenizer.tokenize(i[sent_idx])) for i in dataset]\n",
    "        self.length = [np.int32(len(i)) for i in self.sentences]\n",
    "        for e in range(len(self.sentences)):\n",
    "            self.sentences[e] = padding(self.sentences[e], tokenizer.convert_tokens_to_ids(['PAD']), max_len, pad)\n",
    "        self.segment = np.zeros((len(self.sentences), max_len))\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.sentences[i], self.length[i], self.segment[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "\n",
    "data_train = BERTDataset(trainSet, 0, 1, tokenizer, max_len, True)\n",
    "data_test = BERTDataset(validSet, 0, 1, tokenizer, max_len, True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)\n",
    "print(\"create dataset\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# KoELECTRA Sentence Classification Model\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=10,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        output = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(),\n",
    "                           attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        first_token_tensor = output[0]\n",
    "\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(first_token_tensor[:, 0])\n",
    "        return self.classifier(out)\n",
    "\n",
    "\n",
    "model = BERTClassifier(electra_model,  dr_rate=0.5).to(device)\n",
    "\n",
    "# ============================================================================\n",
    "# Prepare optimizer and schedule\n",
    "# weight decay, scheduler 사용 x\n",
    "# learning rate = 5e-5 고정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "# scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# ============================================================================\n",
    "# Accuracy 계산 함수\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Loop\n",
    "print(\"loop start\")\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e + 1, batch_id + 1, loss.data.cpu().numpy(),\n",
    "                                                                     train_acc / (batch_id + 1)))\n",
    "    print(\"epoch {} train acc {}\".format(e + 1, train_acc / (batch_id + 1)))\n",
    "    PATH = f\"./output/2/koelectrabert_epoch{e+1}.pt\"\n",
    "    torch.save(model, PATH)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length = valid_length\n",
    "            label = label.long().to(device)\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_acc += calc_accuracy(out, label)\n",
    "        print(\"epoch {} valid loss {} acc {}\\n\".format(e + 1, test_loss / (batch_id + 1), test_acc / (batch_id + 1)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algorima_venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
